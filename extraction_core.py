# extraction_core.py
# è²¬å‹™: ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºï¼ˆæ­£è¦è¡¨ç¾é©ç”¨ï¼‰ã¨ã€GUIã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã®æä¾›ã€‚

import pandas as pd
import re
import math # å˜é‡‘å‡¦ç†ã®ãŸã‚ã«mathã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import datetime             # ğŸ’¡ è¿½åŠ : æ—¥ä»˜å‡¦ç†ç”¨
from config import MASTER_COLUMNS, ITEM_PATTERNS, PROCESS_KEYWORDS
# ğŸ“Œ ä¿®æ­£1: configã‹ã‚‰é«˜åº¦ãªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã®æ­£è¦è¡¨ç¾ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from config import RE_AGE_PATTERNS, RE_TANAKA_KW_PATTERNS, RE_TANAKA_RAW_PATTERNS, KEYWORD_TANAKA

# å®Ÿåƒé–‹å§‹æ—¥ã®å„ªå…ˆåº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å®šç¾© (å€¤ãŒå°ã•ã„ã»ã©é«˜å„ªå…ˆåº¦)
DATE_RANKING = {
    'DATE_FULL': 1,      # yyyyå¹´mmæœˆå½¢å¼ (ä¾‹: 202511)
    'DURATION': 2,       # Nãƒ¶æœˆ, å³æ—¥, asap
    'ADJUSTMENT': 3,     # è¦èª¿æ•´, è¦ç›¸è«‡, èª¿æ•´
    'OTHER': 4           # ãã®ä»–
}

def get_target_year(target_month):
    """4ãƒ¶æœˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è€ƒæ…®ã—ã¦å¹´ã‚’è£œå®Œã™ã‚‹"""
    now = datetime.datetime.now()
    current_year = now.year
    current_month = now.month
    
    target_year = current_year
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæœˆãŒç¾åœ¨ã®æœˆã‚ˆã‚Šéå»ã®å ´åˆã€æ¥å¹´ã¨è¦‹ãªã™ 
    if target_month < current_month:
        target_year += 1
        
    return str(target_year)

def process_start_date(date_str):
    """
    å®Ÿåƒé–‹å§‹æ—¥ã®å¾Œå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã€‚
    - yyyy/mm/dd, yyyy-mm, mm/dd, yyyymm å½¢å¼ã‚’ yyyymm å½¢å¼ã«æ­£è¦åŒ–ã™ã‚‹ã€‚
    - 4ãƒ¶æœˆä»¥ä¸Šã®æœªæ¥æ—¥ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€‚
    - æœ€çµ‚çš„ã« 'YYYYMM', 'å³æ—¥', 'Nãƒ¶æœˆ', 'è¦èª¿æ•´', 'n/a' ã®ã„ãšã‚Œã‹ã®ã¿ã‚’è¿”ã™ã€‚
    """
    if pd.isna(date_str) or date_str is None: return 'nan', DATE_RANKING['OTHER']
    
    date_str_lower = date_str.lower().strip() 
    
    # --- 4ãƒ¶æœˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’å®šç¾© ---
    def is_within_4_months(year_str, month_str):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ—¥(YYYYMM)ãŒç¾åœ¨æœˆã‚’å«ã‚4ãƒ¶æœˆä»¥å†…(0ï½4ãƒ¶æœˆ)ã§ã‚ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
        try:
            target_year = int(year_str)
            target_month = int(month_str)
        except ValueError:
            return False 

        now = datetime.datetime.now()
        current_year = now.year
        current_month = now.month

        current_total_months = current_year * 12 + current_month
        target_total_months = target_year * 12 + target_month
        
        month_difference = target_total_months - current_total_months
        
        # 0ãƒ¶æœˆï¼ˆç¾åœ¨æœˆï¼‰ã‹ã‚‰4ãƒ¶æœˆå…ˆï¼ˆ+4ï¼‰ã¾ã§ã‚’è¨±å¯ã—ã€éå»ã®æ—¥ä»˜ï¼ˆ< 0ï¼‰ã¯é™¤å¤–
        if month_difference > 4 or month_difference < 0:
            return False 
        
        return True
        
    # --- 1. â—‹ãƒ¶æœˆã®å‡¦ç† ---
    month_match = re.search(r'([0-9]{1,3})[ãƒ¶ã‹]æœˆ', date_str_lower)
    if month_match: return f"{month_match.group(1)}ãƒ¶æœˆ", DATE_RANKING['DURATION'] 
    
    # --- 2. å³æ—¥/ASAPã®å‡¦ç† ---
    if 'å³æ—¥' in date_str_lower or 'asap' in date_str_lower or re.match(r'^å³[ï½~]?$', date_str_lower): 
        return 'å³æ—¥', DATE_RANKING['DURATION']
    
    # --- 3. yyyy[åŒºåˆ‡ã‚Šæ–‡å­—]mm... å½¢å¼ã®å‡¦ç† (DATE_FULL) ---
    date_match = re.search(r'(\d{4})[\s\./\-å¹´](\d{1,2})', date_str_lower)
    if date_match:
        year = date_match.group(1)
        month = date_match.group(2).zfill(2)
        
        if 1 <= int(month) <= 12:
            # ğŸ“Œ ä¿®æ­£1: 4ãƒ¶æœˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
            if is_within_4_months(year, month): 
                return f"{year}{month}", DATE_RANKING['DATE_FULL']
            return 'nan', DATE_RANKING['OTHER'] 
            
    # 3.3. YYYYMMå½¢å¼ã®å‡¦ç† (åŒºåˆ‡ã‚Šæ–‡å­—ãªã—ã®6æ¡æ•°å­—)
    date_6digit_match = re.search(r'^(\d{6})$', date_str_lower)
    if date_6digit_match:
        year_str = date_6digit_match.group(1)[:4]
        month_str = date_6digit_match.group(1)[4:6]
        try:
             year = int(year_str)
             month = int(month_str)
             if 1 <= month <= 12 and 2000 <= year <= (datetime.datetime.now().year + 5):
                 # ğŸ“Œ ä¿®æ­£2: 4ãƒ¶æœˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
                 if is_within_4_months(year_str, month_str): 
                     return f"{year_str}{month_str}", DATE_RANKING['DATE_FULL']
                 return 'nan', DATE_RANKING['OTHER'] 
        except ValueError:
             pass

    # 3.4. Dayã‚’å«ã‚€å¹´ãªã—ã®æ—¥ä»˜å½¢å¼ã®å‡¦ç†
    date_partial_match_day = re.search(r'(\d{1,2})[\s\./\-](\d{1,2})[\sï½~-]?.*', date_str_lower)
    if date_partial_match_day:
        month = int(date_partial_match_day.group(1))
        
        if 1 <= month <= 12: 
            target_year_str = get_target_year(month) 
            target_month_str = str(month).zfill(2)
            
            # ğŸ“Œ ä¿®æ­£3: 4ãƒ¶æœˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
            if is_within_4_months(target_year_str, target_month_str): 
                return f"{target_year_str}{target_month_str}", DATE_RANKING['DATE_FULL']
            return 'nan', DATE_RANKING['OTHER']

    # --- 4. mmæœˆï¼ˆå¹´ãªã—ï¼‰ã®å‡¦ç† (ä¾‹: 10æœˆ~) ---
    month_only_match = re.search(r'(\d{1,2})æœˆ', date_str_lower) 
    if month_only_match:
        target_month = int(month_only_match.group(1))
        
        if 1 <= target_month <= 12:
            target_year_str = get_target_year(target_month) 
            target_month_str = str(target_month).zfill(2)
                
            # ğŸ“Œ ä¿®æ­£4: 4ãƒ¶æœˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
            if is_within_4_months(target_year_str, target_month_str): 
                return f"{target_year_str}{target_month_str}", DATE_RANKING['DATE_FULL']
            return 'nan', DATE_RANKING['OTHER']

    # --- 5. 4æ¡ã®æ•°å­—ã®ã¿ã®å‡¦ç†ï¼ˆå¹´ã®ã¿ã®æ•°å­—ã‚’n/aã«ã™ã‚‹ï¼‰ ---
    if re.match(r'^\d{4}$', date_str_lower):
        return 'nan', DATE_RANKING['OTHER'] 

    # --- 6. ãã®ä»–ã®èª¿æ•´ãŒå¿…è¦ãªã‚‚ã®ãªã© ---
    if 'èª¿æ•´' in date_str_lower or 'ç›¸è«‡' in date_str_lower or 'è¦' in date_str_lower:
        return 'è¦èª¿æ•´', DATE_RANKING['ADJUSTMENT']
        
    return 'nan', DATE_RANKING['OTHER']


def check_start_date_prefix(match, text_processed, col):
    """æŠ½å‡ºã•ã‚ŒãŸæœŸé–“_é–‹å§‹å€™è£œã«å¯¾ã—ã¦ã€å‰10æ–‡å­—ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€‚"""
    extracted_value = match.group(1).strip()
    start_index_of_match = match.start(0) 
    
    prefix_start_cut = max(0, start_index_of_match - 10) 
    extracted_prefix_10 = text_processed[prefix_start_cut:start_index_of_match]
    
    start_keywords = ['å‚ç”»', 'ç¨¼åƒ', 'ç¨¼å‹•', 'å®Ÿåƒ', 'é–‹å§‹', 'å…¥å ´', 'æ™‚æœŸ', 'start']
    
    is_keyword_present = any(kw in extracted_prefix_10.lower() for kw in start_keywords)
    
    if is_keyword_present:
        # NOTE: match.start(1) ã¯ã‚­ãƒ£ãƒ—ãƒãƒ£ã‚°ãƒ«ãƒ¼ãƒ—1ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        return extracted_value, col, match.start(1)
    
    return None, None, None

def find_start_date_info(row):
    """æœŸé–“_é–‹å§‹ã®æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    text_cols = ['æœ¬æ–‡(ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼)', 'ä»¶å'] 
    
    # --- æœŸé–“_é–‹å§‹ã®æ­£è¦è¡¨ç¾ (ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯) ---
    START_KEYWORDS = (
        r'(?:å‚\s*ç”»|ç¨¼\s*[åƒå‹•]\s*æ—¥?|å®Ÿ\s*åƒ|é–‹\s*å§‹|å…¥\s*å ´|æ™‚\s*æœŸ|start)' 
    )
    DATE_PATTERN_FULL_AND_PARTIAL = (
        r'(?:\d{4}[\s\./\-å¹´]\d{1,2}[\s\./\-æœˆ]\d{1,2}æ—¥?|\d{4}[\s\./\-å¹´]\d{1,2}æœˆ)' 
        r'[\sï½~-]*' 
    )
    ALL_START_DATE_OPTIONS = (
        DATE_PATTERN_FULL_AND_PARTIAL +           
        r'|'
        r'\d{6}'                                
        r'|'
        r'\d{1,2}[\s\./\-]\d{1,2}[\sï½~-]*'      
        r'|'
        r'[1-9][0-9]{0,2}[ãƒ¶ã‹]æœˆ'               
        r'|'
        r'å³æ—¥[\sï½~-]*'                          
        r'|'
        r'asap[\sï½~-]*'                          
        r'|'
        r'å³[ï½~]?'                              
        r'|'
        r'\d{1,2}æœˆ\b'                           
        r'|'
        r'\d{1,2}\b'                             
        r'|'
        r'(?:è¦èª¿æ•´|è¦ç›¸è«‡|èª¿æ•´)'                 
    )
    RE_START_DATE_KEYWORDED = START_KEYWORDS + (
        r'[\s:ï¼šã€\[ï¼ˆ\(]?'                       
        r'(?:å¯\s*èƒ½\s*æ—¥?|\s*æ—¥|\s*æ™‚æœŸ|\s*äºˆå®š)?' 
        r'[ã€‘\]ï¼‰\)]?'                            
        r'[\s:ï¼š]*'                              
        r'('                                      
        r'(?:' + ALL_START_DATE_OPTIONS + r')'    
        r'(?:[\s/,ã€ãƒ»]+'                         
        r'(?:' + ALL_START_DATE_OPTIONS + r'))*'  
        r')'                                      
    )

    RE_START_DATE_RAW = (
        r'(?:\D|^)' 
        r'('         
        + DATE_PATTERN_FULL_AND_PARTIAL +           
        r'|'
        r'\d{6}'                                
        r'|'
        r'\d{1,2}[\s\./\-]\d{1,2}[\sï½~-]*'      
        r'|'
        r'[1-9][0-9]{0,2}[ãƒ¶ã‹]æœˆ'       
        r'|'
        r'å³æ—¥[\sï½~-]*'                          
        r'|'
        r'asap[\sï½~-]*'                          
        r'|'
        r'å³[ï½~]?'                              
        r'|'
        r'\d{1,2}æœˆ\b'                           
        r'|'
        r'\d{1,2}\b'                             
        r'|'
        r'(?:è¦èª¿æ•´|è¦ç›¸è«‡|èª¿æ•´)'         
        r')'
        r'(?:\D|$)' 
    )
    # ----------------------------------------------------------------------
    
    all_candidates = [] 
    keyword_patterns = [RE_START_DATE_KEYWORDED] 
    
    for col in text_cols: 
        text = row.get(col) 
        if pd.isna(text) or text is None: continue
        text_processed = str(text).replace('ã€€', ' ')
        
        for regex in keyword_patterns: 
            matches = re.finditer(regex, text_processed, re.IGNORECASE)
            for match in matches:
                extracted_value = match.group(1).strip()
                index = match.start(1)
                # è¤‡æ•°ã®æ—¥ä»˜å€™è£œï¼ˆä¾‹: 7æœˆ, 8æœˆï¼‰ãŒæŠ½å‡ºã•ã‚ŒãŸå ´åˆã«åˆ†å‰²
                sub_candidates = re.split(r'[\s/,ã€ãƒ»]+', extracted_value) 
                
                for sub_value in sub_candidates:
                    if sub_value.strip():
                         all_candidates.append({
                            'value': sub_value.strip(), 
                            'col': col, 
                            'index': index, 
                            'source': 'KEYWORDED'
                        })
                    
        raw_patterns = [RE_START_DATE_RAW] 

        for regex in raw_patterns: 
            matches = re.finditer(regex, text_processed, re.IGNORECASE)
            
            for match in matches:
                # RAWãƒ‘ã‚¿ãƒ¼ãƒ³ã¯å‰10æ–‡å­—ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                value, src, idx = check_start_date_prefix(match, text_processed, col)
                
                if value:
                    all_candidates.append({
                        'value': value, 
                        'col': src, 
                        'index': idx, 
                        'source': 'RAW_FILTERED'
                    })

    if not all_candidates:
        return None, None, None

    # ãƒ©ãƒ³ã‚¯ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦æœ€é©ãªå€™è£œã‚’æ±ºå®š
    best_match = None
    best_rank = DATE_RANKING['OTHER'] + 1 
    best_index = -1
    
    for candidate in all_candidates:
        # æ­£è¦åŒ–ã¨4ãƒ¶æœˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€ãƒ©ãƒ³ã‚¯ä»˜ã‘
        processed_value, current_rank = process_start_date(candidate['value'])
        
        if processed_value == 'n/a' and current_rank == DATE_RANKING['OTHER']:
            continue # ç„¡åŠ¹ãªå€™è£œã¯ã‚¹ã‚­ãƒƒãƒ—
        
        current_index = candidate['index']
        
        if current_rank < best_rank:
            # ã‚ˆã‚Šé«˜å„ªå…ˆåº¦ã®ãƒ©ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            best_rank = current_rank
            best_match = candidate
            best_match['processed_value'] = processed_value
            best_index = current_index
        
        elif current_rank == best_rank:
            # åŒã˜ãƒ©ãƒ³ã‚¯ã®å ´åˆã€ã‚ˆã‚Šå¾Œã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæœ€æ–°ã®æƒ…å ±ï¼‰ã‚’å„ªå…ˆ
            if current_index > best_index: 
                best_match = candidate
                best_match['processed_value'] = processed_value
                best_index = current_index

    if best_match:
        return best_match['processed_value'], best_match['col'], best_match['index']
    
    return None, None, None

# --- å¹´é½¢/å˜é‡‘ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»å¾Œå‡¦ç†ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’å®šç¾© ---

def check_age_and_prefix(match, text_processed, item_name):
    """ã€å¹´é½¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€‘å‰20æ–‡å­—ã«ã€Œé½¢ã€ã¾ãŸã¯ã€Œåã€ãŒã‚ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    extracted_value = match.group(1).strip()
    start_index = match.start(1)
    
    age_start_index = match.start(1)
    prefix_start_cut = max(0, age_start_index - 20) 
    extracted_prefix_20 = text_processed[prefix_start_cut:age_start_index]
    
    if 'é½¢' in extracted_prefix_20 or 'å' in extracted_prefix_20:
        # ã‚¹ã‚³ã‚¢100ã‚’ä»˜ã‘ã¦è¿”ã™ï¼ˆä»–ã®æ­£è¦è¡¨ç¾ã¨åŒºåˆ¥ã™ã‚‹ãŸã‚ï¼‰
        return extracted_value, 100, start_index
    
    return None, None, None

def check_tanaka_and_prefix(match, text_processed, item_name):
    """ã€å˜é‡‘ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€‘å‰10æ–‡å­—ã«ã€Œå˜ã€ã¾ãŸã¯ã€Œé‡‘é¡ã€ãŒã‚ã‚‹ã‹ã€IDã‚’é™¤å¤–ã™ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    extracted_value = match.group(1).strip()
    start_index = match.start(1)
    
    # ID/URLé™¤å¤–ãƒ­ã‚¸ãƒƒã‚¯ (å‰15æ–‡å­—)
    check_end = match.start(0)
    check_start = max(0, check_end - 15) 
    prefix = text_processed[check_start:check_end]
    if re.search(r'p\?t=M|ã€IDã€‘|\[ID\]|ID[\s:ï¼š]', prefix, re.IGNORECASE):
        return None, None, None 

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿‘æ¥ãƒã‚§ãƒƒã‚¯ (å‰10æ–‡å­—)
    tanaka_start_index = match.start(1)
    prefix_start_cut = max(0, tanaka_start_index - 10) # å‰10æ–‡å­—
    extracted_prefix_10 = text_processed[prefix_start_cut:tanaka_start_index]
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶: ã€Œå˜ã€å˜ç‹¬ã€ã¾ãŸã¯ã€Œé‡‘ã€ã¨ã€Œé¡ã€ã®ã‚»ãƒƒãƒˆ
    is_tanaka_or_kin_gaku = 'å˜' in extracted_prefix_10 or ('é‡‘' in extracted_prefix_10 and 'é¡' in extracted_prefix_10)
    
    if is_tanaka_or_kin_gaku:
        # ã‚¹ã‚³ã‚¢90ã‚’ä»˜ã‘ã¦è¿”ã™ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»˜ã(100)ã‚ˆã‚Šä½ãè¨­å®šï¼‰
        return extracted_value, 90, start_index
    
    return None, None, None

def process_tanaka(tanaka_str: str) -> str:
    """å˜é‡‘ã®å¾Œå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯: ç¯„å›²æŒ‡å®šã¯ãã®ã¾ã¾ã€å˜ä¸€å€¤ã¯ä¸‡å˜ä½ã«å¤‰æ›ã—ã€åˆ‡ã‚Šä¸Šã’"""
    if not tanaka_str or pd.isna(tanaka_str): return 'nan'
    tanaka_str = str(tanaka_str).lower().replace(' ', '').replace(',', '')
    
    # 1. ç¯„å›²æŒ‡å®šã®å‡¦ç†
    if ('~' in tanaka_str or 'ï½' in tanaka_str or '-' in tanaka_str):
        range_str = tanaka_str.replace('ä¸‡', '').replace('å††', '').replace('~', 'ï½').replace('-', 'ï½')
        parts = re.split(r'ï½', range_str)
        if all(re.match(r'^\d+(\.\d+)?$', part) for part in parts) and len(parts) == 2:
             return range_str
        return 'nan'
    
    # 2. å˜ä¸€å€¤ã®å‡¦ç†
    man_value = None
    if 'ä¸‡' in tanaka_str:
        num_part = tanaka_str.replace('ä¸‡', '')
        try: man_value = float(num_part)
        except ValueError: pass
    elif 'å††' in tanaka_str:
        num_part = tanaka_str.replace('å††', '')
        if re.match(r'^\d+$', num_part): 
            num = int(num_part)
            man_value = num / 10000.0 if num >= 10000 else None
            if man_value is None: return str(num)
    elif re.match(r'^\d+$', tanaka_str): 
        num = int(tanaka_str)
        man_value = num / 10000.0 if num >= 10000 else None
        if man_value is None: return str(num)

    # 3. ç¹°ã‚Šä¸Šã’å‡¦ç† (ä¸‡å˜ä½ã®ã¿)
    if man_value is not None:
        # ä¾‹: 70.5ä¸‡ -> 71ä¸‡ ã¨ãªã‚‹ã‚ˆã†ã«åˆ‡ã‚Šä¸Šã’
        return str(int(math.ceil(man_value)))

    return 'nan'


def clean_and_normalize(value: str, item_name: str) -> str:
    """æŠ½å‡ºçµæœã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã€æ­£è¦åŒ–ã™ã‚‹é–¢æ•°ã€‚ï¼ˆãƒã‚¤ã‚ºé™¤å»ã‚’å«ã‚€ï¼‰"""
    if not value or not value.strip(): 
        return 'nan'
    
    cleaned = value.strip().replace('\xa0', ' ')
    cleaned = re.sub(r'[\s\u3000]+', ' ', cleaned).strip() 
    
    if item_name == 'åå‰' or item_name == 'æ°å':
        cleaned = re.sub(r'[\(ï¼ˆ\[ã€].*?[\)ï¼‰\]ã€‘]', '', cleaned) 
        cleaned = re.sub(r'[ãƒ»\_]', ' ', cleaned)
        cleaned = re.sub(r'[-]+$', '', cleaned).strip() 
        cleaned = re.sub(r'\s+', '', cleaned).strip()
    
    # ğŸ“Œ ä¿®æ­£2: å¹´é½¢ã®å¾Œå‡¦ç†ã¯ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–¢æ•° (check_age_and_prefix) ã§ã®æŠ½å‡ºå¾Œã«è¡Œã†ãŸã‚ã€
    #          ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªæ•´å½¢ã®ã¿ã«ã¨ã©ã‚ã‚‹ï¼ˆä¸»ãªå‡¦ç†ã¯`process_tanaka`ã§è¡Œã†ï¼‰
    if item_name == 'å¹´é½¢':
        # æ•°å­—ã€ãƒã‚¤ãƒ•ãƒ³ã€ãƒãƒ«ãƒ€ä»¥å¤–ã‚’é™¤å»ã—ã€æ•°å­—ã®ã¿ã‚’è¿”ã™ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯extract_skills_dataå†…ã§å®Ÿæ–½æ¸ˆã¿ï¼‰
        cleaned = re.sub(r'[^\d\s\-\-ï½~]', '', cleaned).strip()
        # ç¯„å›²æŒ‡å®šã®æ•°å­—ã‚’å˜ä¸€å€¤ã¨ã¿ãªã—ã€æœ€åˆã®æ•°å­—ã ã‘ã‚’æŠ½å‡ºã™ã‚‹
        match = re.search(r'(\d+)', cleaned)
        return match.group(1) if match else 'N/A'
        
    if item_name == 'å˜é‡‘':
       
        return value.strip() 

    if item_name in ['ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆçµŒé¨“äººæ•°']:
        return re.sub(r'[\D\.,]+', '', cleaned) 
        
    if item_name in ['ã‚¹ã‚­ãƒ«orè¨€èª', 'OS', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯/ãƒ©ã‚¤ãƒ–ãƒ©ãƒª', 'é–‹ç™ºãƒ„ãƒ¼ãƒ«']:
        cleaned = re.sub(r'^ã€\s*è¨€\s*èª\s*ã€‘|^ã€\s*DB\s*ã€‘|^ã€\s*OS\s*ã€‘', '', cleaned, flags=re.IGNORECASE)
        cleaned = cleaned.strip() 
        
        cleaned = re.sub(r'[ãƒ»ã€/\\|,;]+', ',', cleaned) 
        cleaned = re.sub(r'\s*,\s*', ',', cleaned).strip(',') 
        
        if not cleaned:
            return 'N/A'
        
    return cleaned


def extract_skills_data(mail_data_df: pd.DataFrame) -> pd.DataFrame:
    """ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿DataFrameã‚’å—ã‘å–ã‚Šã€æŠ½å‡ºçµæœã¨ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¿”ã™ã€‚"""
    
    SOURCE_TEXT_COL = 'æœ¬æ–‡(æŠ½å‡ºå…ƒçµåˆ)'
    if SOURCE_TEXT_COL not in mail_data_df.columns:
        SOURCE_TEXT_COL = 'æœ¬æ–‡(ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼)'
        
    mail_data_df[SOURCE_TEXT_COL] = mail_data_df[SOURCE_TEXT_COL].astype(str).str.replace(r'[\r\n\t"]', ' ', regex=True)
    mail_data_df[SOURCE_TEXT_COL] = mail_data_df[SOURCE_TEXT_COL].astype(str).str.replace(r'\s+', ' ', regex=True)
    
    all_extracted_rows = []
    
    for index, row in mail_data_df.iterrows():
        mail_id = str(row.get('EntryID', f'Row_{index+1}'))
        full_text_for_search = str(row.get(SOURCE_TEXT_COL, ''))
        full_text_for_search = re.sub(r'\[FILE (ERROR|WARN):.*?\]', '', full_text_for_search)
        text_processed = full_text_for_search # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã¨ã—ã¦ä¿æŒ
        
        mail_body_for_display = str(row.get('æœ¬æ–‡(ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼)', 'N/A'))
        file_body_for_display = str(row.get('æœ¬æ–‡(ãƒ•ã‚¡ã‚¤ãƒ«å«ã‚€)', 'N/A'))

        extracted_data = {'EntryID': mail_id, 'ä»¶å': row.get('ä»¶å', 'N/A'), 'å®›å…ˆãƒ¡ãƒ¼ãƒ«': row.get('å®›å…ˆãƒ¡ãƒ¼ãƒ«', 'N/A')} 
        reliability_scores = {} 
        
        # --- 2.1. é«˜åº¦ãªå¹´é½¢æŠ½å‡º (æœ€å„ªå…ˆ) ---
        age_extracted_value, age_score = 'nan', 0
        for regex in RE_AGE_PATTERNS:
            match = re.search(regex, text_processed, re.IGNORECASE)
            if match:
                value, score, _ = check_age_and_prefix(match, text_processed, 'å¹´é½¢')
                if value and score > age_score:
                    age_extracted_value = value
                    age_score = score
                    break # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸç¢ºåº¦ã®é«˜ã„ã‚‚ã®æ¡ç”¨
                    
        if age_extracted_value != 'nan':
            # clean_and_normalizeã§å˜ä¸€å€¤ã®æ•°å­—ã®ã¿ã‚’æŠ½å‡º
            extracted_data['å¹´é½¢'] = clean_and_normalize(age_extracted_value, 'å¹´é½¢')
            reliability_scores['å¹´é½¢'] = age_score
        
        # --- 2.2. é«˜åº¦ãªå˜é‡‘æŠ½å‡º (æœ€å„ªå…ˆ) ---
        tanaka_extracted_value, tanaka_score = 'nan', 0
        
        # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³ (RE_TANAKA_KW_PATTERNS) ã‚’å„ªå…ˆé †ä½é€šã‚Šã«ãƒã‚§ãƒƒã‚¯
        for regex in RE_TANAKA_KW_PATTERNS:
            match = re.search(regex, text_processed, re.IGNORECASE)
            if match:
                tanaka_extracted_value = match.group(1).strip()
                tanaka_score = 100 # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»˜ãã¯é«˜ã‚¹ã‚³ã‚¢
                break # å„ªå…ˆåº¦ã®é«˜ã„ã‚‚ã®ãŒæ¡ç”¨ã•ã‚ŒãŸã‚‰çµ‚äº†
        
        # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»˜ããŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã€RAWãƒ‘ã‚¿ãƒ¼ãƒ³ (RE_TANAKA_RAW_PATTERNS) ã‚’å„ªå…ˆé †ä½é€šã‚Šã«ãƒã‚§ãƒƒã‚¯ (ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚ã‚Š)
        if tanaka_extracted_value == 'nan':
            for regex in RE_TANAKA_RAW_PATTERNS:
                matches = re.finditer(regex, text_processed, re.IGNORECASE)
                for match in matches:
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ã‚’é©ç”¨
                    value, score, _ = check_tanaka_and_prefix(match, text_processed, 'å˜é‡‘')
                    if value and score > tanaka_score:
                        tanaka_extracted_value = value
                        tanaka_score = score
                        break # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸç¢ºåº¦ã®é«˜ã„ã‚‚ã®æ¡ç”¨

        # å¾Œå‡¦ç†ã®é©ç”¨
        extracted_data['å˜é‡‘'] = process_tanaka(tanaka_extracted_value)
        if extracted_data['å˜é‡‘'] != 'nan':
            reliability_scores['å˜é‡‘'] = tanaka_score if tanaka_score > 0 else 100
            
        # --- 2.3. æœŸé–“_é–‹å§‹ ã®é«˜åº¦ãªæŠ½å‡ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
        start_date_value, _, _ = find_start_date_info(row)
        if start_date_value and start_date_value != 'nan':
             extracted_data['æœŸé–“_é–‹å§‹'] = start_date_value
             reliability_scores['æœŸé–“_é–‹å§‹'] = 100 
            
        # --- 2.4. ãã®ä»–ã®é …ç›®ã®æŠ½å‡º ---
        for item_key, pattern_info in ITEM_PATTERNS.items():
            base_item_name = item_key.split('_')[0]
            
            # å¹´é½¢ã€å˜é‡‘ã€æœŸé–“ã¯ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯ã§å‡¦ç†æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
            if base_item_name in ['å¹´é½¢', 'å˜é‡‘', 'æœŸé–“']:
                continue
            
            pattern = pattern_info['pattern'] 
            flags = re.IGNORECASE
            if item_key == 'ã‚¹ã‚­ãƒ«orè¨€èª': flags |= re.DOTALL 

            match = re.search(pattern, full_text_for_search, flags)
            
            if match:
                extracted_value = match.group(1) if match.groups() else match.group(0)
                score = pattern_info['score']
                
                cleaned_val = clean_and_normalize(extracted_value, base_item_name)
                
                current_score = reliability_scores.get(base_item_name, 0)
                if score > current_score:
                    extracted_data[base_item_name] = cleaned_val
                    reliability_scores[base_item_name] = score
            
        # --- 3. é–‹ç™ºå·¥ç¨‹ãƒ•ãƒ©ã‚°ã®åˆ¤å®š (å¤‰æ›´ãªã—) ---
        for proc_name, keywords in PROCESS_KEYWORDS.items():
            flag_col = f'é–‹ç™ºå·¥ç¨‹_{proc_name}'
            extracted_data[flag_col] = 'ãªã—' 
            if re.search('|'.join(keywords), full_text_for_search, re.IGNORECASE):
                extracted_data[flag_col] = 'ã‚ã‚Š' 

        # --- 4. æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰ã¨è£œå®Œ (å¤‰æ›´ãªã—) ---
        final_row = {} 
        final_row.update(extracted_data) 

        final_row['æœ¬æ–‡(ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼)'] = mail_body_for_display
        final_row['æœ¬æ–‡(ãƒ•ã‚¡ã‚¤ãƒ«å«ã‚€)'] = file_body_for_display 
        
        attachments_list = row.get('Attachments', [])
        if isinstance(attachments_list, list):
            final_row['Attachments'] = ', '.join(attachments_list)
        else:
            final_row['Attachments'] = str(attachments_list)

        valid_scores = [s for s in reliability_scores.values() if s > 0]
        final_row['ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢'] = round(sum(valid_scores) / len(valid_scores) if valid_scores else 0, 1)

        all_extracted_rows.append(final_row)
            
    # æœ€çµ‚DataFrameã®æ§‹ç¯‰
    df_extracted = pd.DataFrame(all_extracted_rows)
    
    if 'æœ¬æ–‡(æŠ½å‡ºå…ƒçµåˆ)' in df_extracted.columns:
        df_extracted = df_extracted.drop(columns=['æœ¬æ–‡(æŠ½å‡ºå…ƒçµåˆ)']) 
        
    all_cols_in_order = [
        'EntryID', 'ä»¶å', 'å®›å…ˆãƒ¡ãƒ¼ãƒ«', 'æœ¬æ–‡(ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼)', 'æœ¬æ–‡(ãƒ•ã‚¡ã‚¤ãƒ«å«ã‚€)', 
        'Attachments', 'ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢'
    ] + [col for col in MASTER_COLUMNS if col not in ['EntryID', 'ä»¶å', 'å®›å…ˆãƒ¡ãƒ¼ãƒ«', 'æœ¬æ–‡(ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼)', 'æœ¬æ–‡(ãƒ•ã‚¡ã‚¤ãƒ«å«ã‚€)', 'Attachments', 'ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢', 'æœ¬æ–‡(æŠ½å‡ºå…ƒçµåˆ)']]
    
    df_extracted = df_extracted.reindex(columns=[c for c in all_cols_in_order if c in df_extracted.columns], fill_value='N/A')
    df_extracted = df_extracted.astype(str)
    
    return df_extracted